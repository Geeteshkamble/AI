{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Geeteshkamble/AI/blob/main/nb/Gemma3_(4B).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from unsloth import FastModel\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from unsloth.chat_templates import get_chat_template, standardize_data_formats, train_on_responses_only\n",
        "from transformers import TextStreamer\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVpJtnXzXERU",
        "outputId": "99881fe1-3f33-4c5c-b16f-f070e5d1f815"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "INFO 04-30 06:37:30 [importing.py:53] Triton module has been replaced with a placeholder.\n",
            "INFO 04-30 06:37:30 [__init__.py:239] Automatically detected platform cuda.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Available models for training\n",
        "fourbit_models = [\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
        "]"
      ],
      "metadata": {
        "id": "lzNXFj_CXGSb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_colab():\n",
        "    \"\"\"Setup Colab environment with required dependencies.\"\"\"\n",
        "    if \"COLAB_\" in \"\".join(os.environ.keys()):\n",
        "        print(\"Setting up Colab environment...\")\n",
        "        # Install dependencies\n",
        "        !pip install --no-deps unsloth vllm\n",
        "        !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo\n",
        "        !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "\n",
        "        # Handle vLLM requirements\n",
        "        f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "        with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "            file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "        !pip install -r vllm_requirements.txt\n",
        "\n",
        "        # Clear some modules to avoid conflicts\n",
        "        modules = list(sys.modules.keys())\n",
        "        for x in modules:\n",
        "            if \"PIL\" in x or \"google\" in x:\n",
        "                sys.modules.pop(x)\n",
        "\n",
        "        print(\"Colab environment setup complete!\")"
      ],
      "metadata": {
        "id": "RBak07mAXWt5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_prepare_dataset(csv_path):\n",
        "    \"\"\"Load and prepare the negative words dataset.\"\"\"\n",
        "    try:\n",
        "        # Read the CSV file with explicit column names\n",
        "        df = pd.read_csv(csv_path, encoding='utf-8', names=['Child', 'Robot'])\n",
        "\n",
        "        # Print first few rows for debugging\n",
        "        print(\"\\nFirst few rows of the dataset:\")\n",
        "        print(df.head())\n",
        "\n",
        "        # Create conversations format\n",
        "        conversations = []\n",
        "        for _, row in df.iterrows():\n",
        "            # Skip empty rows or header row\n",
        "            if pd.isna(row['Child']) or pd.isna(row['Robot']) or row['Child'] == 'Child':\n",
        "                continue\n",
        "\n",
        "            # Clean and validate the text\n",
        "            child_text = str(row['Child']).strip()\n",
        "            robot_text = str(row['Robot']).strip()\n",
        "\n",
        "            if child_text and robot_text:  # Only add if both texts are non-empty\n",
        "                conversation = [\n",
        "                    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": child_text}]},\n",
        "                    {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": robot_text}]}\n",
        "                ]\n",
        "                conversations.append({\"conversations\": conversation})\n",
        "\n",
        "        if not conversations:\n",
        "            raise ValueError(\"No valid conversations found in the dataset\")\n",
        "\n",
        "        # Convert to HuggingFace dataset\n",
        "        dataset = Dataset.from_list(conversations)\n",
        "        print(f\"\\nSuccessfully created {len(conversations)} conversation pairs\")\n",
        "        return dataset\n",
        "\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(\"Error: The CSV file is empty\")\n",
        "        raise\n",
        "    except pd.errors.ParserError:\n",
        "        print(\"Error: The CSV file is not properly formatted\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {str(e)}\")\n",
        "        print(\"\\nPlease ensure your CSV file:\")\n",
        "        print(\"1. Has two columns (negative statement and positive response)\")\n",
        "        print(\"2. Is properly formatted with commas as separators\")\n",
        "        print(\"3. Contains valid text in both columns\")\n",
        "        raise\n"
      ],
      "metadata": {
        "id": "APB-nfdzXYuY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dataset(dataset, tokenizer):\n",
        "    \"\"\"Format the dataset using the chat template.\"\"\"\n",
        "    def formatting_prompts_func(examples):\n",
        "        convos = examples[\"conversations\"]\n",
        "        texts = []\n",
        "        for convo in convos:\n",
        "            # Format each conversation into a single text string\n",
        "            formatted_text = \"\"\n",
        "            for message in convo:\n",
        "                role = message[\"role\"]\n",
        "                content = message[\"content\"][0][\"text\"]\n",
        "                if role == \"user\":\n",
        "                    formatted_text += f\"<start_of_turn>user\\n{content}<end_of_turn>\\n\"\n",
        "                else:  # assistant\n",
        "                    formatted_text += f\"<start_of_turn>model\\n{content}<end_of_turn>\\n\"\n",
        "            texts.append(formatted_text)\n",
        "        return {\"text\": texts}\n",
        "\n",
        "    return dataset.map(formatting_prompts_func, batched=True)"
      ],
      "metadata": {
        "id": "OA6rIuKOXbgg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(dataset, model_name=\"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\", max_steps=60):\n",
        "    \"\"\"Train the Gemma-3 model on the dataset.\"\"\"\n",
        "    # Initialize model and tokenizer\n",
        "    model, tokenizer = FastModel.from_pretrained(\n",
        "        model_name=model_name,\n",
        "        max_seq_length=2048,  # Choose any for long context!\n",
        "        load_in_4bit=True,    # 4 bit quantization to reduce memory\n",
        "        load_in_8bit=False,   # A bit more accurate, uses 2x memory\n",
        "        full_finetuning=False, # We have full finetuning now!\n",
        "    )\n",
        "\n",
        "    # Get PEFT model\n",
        "    model = FastModel.get_peft_model(\n",
        "        model,\n",
        "        finetune_vision_layers=False,  # Turn off for just text!\n",
        "        finetune_language_layers=True,  # Should leave on!\n",
        "        finetune_attention_modules=True,  # Attention good for GRPO\n",
        "        finetune_mlp_modules=True,  # Should leave on always!\n",
        "        r=8,           # Larger = higher accuracy, but might overfit\n",
        "        lora_alpha=8,  # Recommended alpha == r at least\n",
        "        lora_dropout=0,\n",
        "        bias=\"none\",\n",
        "        random_state=3407,\n",
        "    )\n",
        "\n",
        "    # Format dataset\n",
        "    print(\"\\nFormatting dataset for training...\")\n",
        "    dataset = format_dataset(dataset, tokenizer)\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=dataset,\n",
        "        eval_dataset=None,\n",
        "        args=SFTConfig(\n",
        "            dataset_text_field=\"text\",\n",
        "            per_device_train_batch_size=2,\n",
        "            gradient_accumulation_steps=4,  # Use GA to mimic batch size!\n",
        "            warmup_steps=5,\n",
        "            max_steps=max_steps,\n",
        "            learning_rate=2e-4,  # Reduce to 2e-5 for long training runs\n",
        "            logging_steps=1,\n",
        "            optim=\"adamw_8bit\",\n",
        "            weight_decay=0.01,\n",
        "            lr_scheduler_type=\"linear\",\n",
        "            seed=3407,\n",
        "            report_to=\"none\",  # Use this for WandB etc\n",
        "            dataset_num_proc=2,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    print(\"\\nStarting training...\")\n",
        "    trainer_stats = trainer.train()\n",
        "\n",
        "    return model, tokenizer, trainer_stats\n"
      ],
      "metadata": {
        "id": "9G4cFLF_Xdly"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, tokenizer, test_inputs):\n",
        "    \"\"\"Test the trained model with sample inputs.\"\"\"\n",
        "    results = []\n",
        "    for input_text in test_inputs:\n",
        "        messages = [{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": input_text}]\n",
        "        }]\n",
        "\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=True,\n",
        "        )\n",
        "\n",
        "        outputs = model.generate(\n",
        "            **tokenizer([text], return_tensors=\"pt\").to(\"cuda\"),\n",
        "            max_new_tokens=200,  # Increase for longer outputs!\n",
        "            temperature=1.0,    # Recommended Gemma-3 settings!\n",
        "            top_p=0.95,         # Recommended Gemma-3 settings!\n",
        "            top_k=64,           # Recommended Gemma-3 settings!\n",
        "        )\n",
        "\n",
        "        response = tokenizer.batch_decode(outputs)[0]\n",
        "        results.append({\"input\": input_text, \"response\": response})\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "BioIU0SFXpcJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Setup Colab environment if running in Colab\n",
        "    setup_colab()\n",
        "\n",
        "    # Load and prepare dataset\n",
        "    print(\"Loading and preparing dataset...\")\n",
        "    try:\n",
        "        dataset = load_and_prepare_dataset(\"Negative words.csv\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nFailed to load dataset: {str(e)}\")\n",
        "        print(\"\\nPlease check your CSV file and try again.\")\n",
        "        return\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\nStarting model training...\")\n",
        "    try:\n",
        "        model, tokenizer, trainer_stats = train_model(dataset)\n",
        "        print(f\"\\nTraining completed in {trainer_stats.metrics['train_runtime']} seconds\")\n",
        "        print(f\"Training completed in {round(trainer_stats.metrics['train_runtime']/60, 2)} minutes\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during training: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # Show memory stats\n",
        "    try:\n",
        "        gpu_stats = torch.cuda.get_device_properties(0)\n",
        "        used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "        print(f\"\\nGPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "        print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "        print(f\"Peak reserved memory % of max memory = {round(used_memory / max_memory * 100, 3)} %.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError getting GPU stats: {str(e)}\")\n",
        "\n",
        "    # Save the model\n",
        "    print(\"\\nSaving model...\")\n",
        "    try:\n",
        "        model.save_pretrained(\"gemma-3-negative-words\")\n",
        "        tokenizer.save_pretrained(\"gemma-3-negative-words\")\n",
        "        print(\"Model saved successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError saving model: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # Test cases\n",
        "    test_inputs = [\n",
        "        # \"I don't want to do my homework!\",\n",
        "        # \"I don't like vegetables!\",\n",
        "        # \"I don't want to go to school today!\",\n",
        "        # \"I don't want to share my toys!\",\n",
        "        # \"I don't want to go to bed!\",\n",
        "        # \"I don't want to clean my room!\",\n",
        "        # \"I don't want to help with dinner!\",\n",
        "        # \"I don't want to play outside!\",\n",
        "        # \"I don't want to take a bath!\",\n",
        "        # \"I don't want to exercise!\"\n",
        "        \"i peed in my pants while sleeping\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\nTesting model with sample inputs:\")\n",
        "    try:\n",
        "        results = test_model(model, tokenizer, test_inputs)\n",
        "\n",
        "        for result in results:\n",
        "            print(f\"\\nInput: {result['input']}\")\n",
        "            print(f\"Response: {result['response']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during testing: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # Save to float16 for deployment\n",
        "    print(\"\\nSaving model in float16 format for deployment...\")\n",
        "    try:\n",
        "        model.save_pretrained_merged(\"gemma-3-negative-words-float16\", tokenizer)\n",
        "        print(\"Float16 model saved successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError saving float16 model: {str(e)}\")\n",
        "\n",
        "    # Save to GGUF format\n",
        "    print(\"\\nSaving model in GGUF format...\")\n",
        "    try:\n",
        "        model.save_pretrained_gguf(\n",
        "            \"gemma-3-negative-words-gguf\",\n",
        "            quantization_type=\"Q8_0\",  # For now only Q8_0, BF16, F16 supported\n",
        "        )\n",
        "        print(\"GGUF model saved successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError saving GGUF model: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "awG5tN9i3vQH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}