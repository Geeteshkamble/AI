{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Geeteshkamble/AI/blob/main/nb/Llama3.1_(8B)-GRPO%2002/05/2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E5P977sBNCFh"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    !pip install --no-deps unsloth vllm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fWGzs0w3NCFi"
      },
      "outputs": [],
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    !pip install --no-deps unsloth vllm\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    # Skip restarting message in Colab\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "\n",
        "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
        "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "    !pip install -r vllm_requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unsloth\n",
        "import pandas as pd\n",
        "from unsloth import FastModel\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from unsloth.chat_templates import get_chat_template, standardize_data_formats, train_on_responses_only\n",
        "from transformers import TextStreamer\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import requests"
      ],
      "metadata": {
        "id": "fzlegaGeN8pB",
        "outputId": "835f27e3-635d-4fd9-efac-2d5337ffff6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "INFO 05-02 08:58:53 [importing.py:53] Triton module has been replaced with a placeholder.\n",
            "INFO 05-02 08:58:53 [__init__.py:239] Automatically detected platform cuda.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DkIvEkIIkEyB"
      },
      "outputs": [],
      "source": [
        "def load_and_prepare_dataset(csv_path):\n",
        "    \"\"\"Load and prepare the dataset.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path, encoding='utf-8', names=['Child', 'Robot'])\n",
        "        conversations = []\n",
        "        for _, row in df.iterrows():\n",
        "            if pd.isna(row['Child']) or pd.isna(row['Robot']) or row['Child'] == 'Child':\n",
        "                continue\n",
        "            child_text = str(row['Child']).strip()\n",
        "            robot_text = str(row['Robot']).strip()\n",
        "            if child_text and robot_text:\n",
        "                conversation = [\n",
        "                    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": child_text}]},\n",
        "                    {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": robot_text}]}\n",
        "                ]\n",
        "                conversations.append({\"conversations\": conversation})\n",
        "        if not conversations:\n",
        "            raise ValueError(\"No valid conversations found in the dataset\")\n",
        "        dataset = Dataset.from_list(conversations)\n",
        "        return dataset\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {str(e)}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dataset(dataset, tokenizer):\n",
        "    \"\"\"Format the dataset using the chat template.\"\"\"\n",
        "    def formatting_prompts_func(examples):\n",
        "        convos = examples[\"conversations\"]\n",
        "        texts = []\n",
        "        for convo in convos:\n",
        "            formatted_text = \"\"\n",
        "            for message in convo:\n",
        "                role = message[\"role\"]\n",
        "                content = message[\"content\"][0][\"text\"]\n",
        "                if role == \"user\":\n",
        "                    formatted_text += f\"<start_of_turn>user\\n{content}<end_of_turn>\\n\"\n",
        "                else:\n",
        "                    formatted_text += f\"<start_of_turn>model\\n{content}<end_of_turn>\\n\"\n",
        "            texts.append(formatted_text)\n",
        "        return {\"text\": texts}\n",
        "    return dataset.map(formatting_prompts_func, batched=True)"
      ],
      "metadata": {
        "id": "ohYLPIXNOLyE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(dataset, model_name=\"meta-llama/meta-Llama-3.1-8B-Instruct\", max_steps=60):\n",
        "    \"\"\"Train the model.\"\"\"\n",
        "    model, tokenizer = FastModel.from_pretrained(\n",
        "        model_name=model_name,\n",
        "        max_seq_length=2048,\n",
        "        load_in_4bit=True,\n",
        "        load_in_8bit=False,\n",
        "        full_finetuning=False,\n",
        "    )\n",
        "    model = FastModel.get_peft_model(\n",
        "        model,\n",
        "        finetune_vision_layers=False,\n",
        "        finetune_language_layers=True,\n",
        "        finetune_attention_modules=True,\n",
        "        finetune_mlp_modules=True,\n",
        "        r=8,\n",
        "        lora_alpha=8,\n",
        "        lora_dropout=0,\n",
        "        bias=\"none\",\n",
        "        random_state=3407,\n",
        "    )\n",
        "    dataset = format_dataset(dataset, tokenizer)\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=dataset,\n",
        "        eval_dataset=None,\n",
        "        args=SFTConfig(\n",
        "            dataset_text_field=\"text\",\n",
        "            per_device_train_batch_size=2,\n",
        "            gradient_accumulation_steps=4,\n",
        "            warmup_steps=5,\n",
        "            max_steps=max_steps,\n",
        "            learning_rate=2e-4,\n",
        "            logging_steps=1,\n",
        "            optim=\"adamw_8bit\",\n",
        "            weight_decay=0.01,\n",
        "            lr_scheduler_type=\"linear\",\n",
        "            seed=3407,\n",
        "            report_to=\"none\",\n",
        "            dataset_num_proc=2,\n",
        "        ),\n",
        "    )\n",
        "    trainer_stats = trainer.train()\n",
        "    return model, tokenizer, trainer_stats"
      ],
      "metadata": {
        "id": "ps2l_f45OM8-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, tokenizer, test_inputs):\n",
        "    \"\"\"Test the trained model with sample inputs.\"\"\"\n",
        "    results = []\n",
        "    for input_text in test_inputs:\n",
        "        messages = [{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": input_text}]\n",
        "        }]\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=True,\n",
        "        )\n",
        "        outputs = model.generate(\n",
        "            **tokenizer([text], return_tensors=\"pt\").to(\"cuda\"),\n",
        "            max_new_tokens=200,\n",
        "            temperature=1.0,\n",
        "            top_p=0.95,\n",
        "            top_k=64,\n",
        "        )\n",
        "        response = tokenizer.batch_decode(outputs)[0]\n",
        "        results.append({\"input\": input_text, \"response\": response})\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "KOBDRrRVOOWt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"Loading and preparing dataset...\")\n",
        "    dataset = load_and_prepare_dataset(\"/content/Negative words.csv\")\n",
        "    print(\"\\nStarting model training...\")\n",
        "    model, tokenizer, trainer_stats = train_model(dataset)\n",
        "    print(f\"\\nTraining completed in {trainer_stats.metrics['train_runtime']} seconds\")\n",
        "    print(\"\\nTesting model with sample inputs:\")\n",
        "    test_inputs = [\n",
        "        \"I don't want to do my homework!\",\n",
        "        \"I don't like vegetables!\",\n",
        "        \"I don't want to go to school today!\",\n",
        "    ]\n",
        "    results = test_model(model, tokenizer, test_inputs)\n",
        "    for result in results:\n",
        "        print(f\"\\nInput: {result['input']}\")\n",
        "        print(f\"Response: {result['response']}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "gEGVt4AkOP-X"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}